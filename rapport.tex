Master 2 Bases de Données et Intelligence Artificielle
Module de Gestion et analyse des Données Massives
Exercice de CC à rendre pour le 21 Mars 2025
À réaliser par groupe de 2 étudiants au maximum
Pour les exercices 2 et 3, vous devez rendre un rapport d’analyse de 5 à 10 pages maximum et
le code source. Ces éléments seront à déposer sur Plubel. Aucun rendu ne sera accepté en dehors du
dépôt sur Plubel. Les questions sont un guide. Vous devez détailler la conception de votre programme
et expliquer les éléments techniques essentiels.
Exercice 2 : Réalisation de programmes clients Java
On cherche à utiliser Apache Kafka depuis un programme Java et à comparer ses fonctionnalités
avec JMS. Utiliser Maven (commande mvn) pour compiler les programmes et exécuter les programmes.
Utiliser les exemples du CM comme point de départ 1
.
1. Compiler le producteur, le lancer. Tester la bonne réception des messages avec le consommateur
en mode console.
2. Compiler le programme consommateur, le tester.
3. Utiliser plusieurs producteurs afin de produire rapidement 106 messages dans un topic ayant 2
partitions.
4. Consommer cette masse de messages en testant deux stratégies : 1) avec les consommateurs dans
un même groupe (tester avec 2 et 3 consommateurs) ; 2) avec des consommateurs de différents
groupes. Mesurer les temps de consommation pour chacun des cas.
5. Comparer les fonctionnalités de consommation de messages avec JMS.
Exercice 3 : Stream processing
On souhaite mettre en place un système de monitoring de données issues de capteurs de température
d’un ensemble de bâtiments.
1. Définir une architecture pour réaliser les fonctionnalités des 2 questions suivantes avec Kafka
en utilisant les principes de Kafka et Kakfa streams.
1. Ils sont adaptés de ceux développés par Gwen Shapira, un des commiteers de Kafka (https://github.com/
gwenshap/kafka-examples)
2. Définir et réaliser un programme multi-producteurs pour générer un flux de données de température. On considère que chaque bâtiment émet des données ayant comme clé le nom du
bâtiment, et comme valeur un ensemble de données constituées du nom de la salle et de sa
température. Les messages sont envoyés toutes les 10 secondes par les bâtiments. Le topic dans
lequel les données sont envoyées doit avoir au moins 2 partitions.
3. Réaliser le programme consommateur avec Kafka streams. Les messages émis pas les bâtiments
doivent être récupérés, puis la moyenne de température pour chaque salle doit être calculée sur
une fenêtre de 5 minutes. Une alerte doit pouvoir être émise lorsque la température d’une salle
passe en-dessous ou au-dessus d’un certain seuil.





\section{Introduction}

\subsection{Objectifs}

L'objectif de ce projet est de réaliser une conception avec Zookeeper et Kafka pour la gestion de données massives.
Nous allons réaliser un programme Java qui gère des flux de données entre des producteurs et des consommateurs.